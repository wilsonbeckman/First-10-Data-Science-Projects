{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae301465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: remotezip in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (0.10.0)\n",
      "Requirement already satisfied: tqdm in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (4.64.1)\n",
      "Collecting opencv-python==4.5.2.52\n",
      "  Using cached opencv_python-4.5.2.52-cp39-cp39-macosx_10_15_x86_64.whl (43.6 MB)\n",
      "Collecting opencv-python-headless==4.5.2.52\n",
      "  Using cached opencv_python_headless-4.5.2.52-cp39-cp39-macosx_10_15_x86_64.whl (43.6 MB)\n",
      "Collecting tf-models-official\n",
      "  Using cached tf_models_official-2.11.2-py2.py3-none-any.whl (2.3 MB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from opencv-python==4.5.2.52) (1.23.4)\n",
      "Requirement already satisfied: requests in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from remotezip) (2.28.1)\n",
      "Requirement already satisfied: tabulate in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from remotezip) (0.9.0)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tf-models-official) (5.9.0)\n",
      "Collecting pycocotools\n",
      "  Using cached pycocotools-2.0.6.tar.gz (24 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-api-python-client>=1.6.7\n",
      "  Using cached google_api_python_client-2.70.0-py2.py3-none-any.whl (10.7 MB)\n",
      "Requirement already satisfied: Pillow in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tf-models-official) (9.3.0)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tf-models-official) (1.4.4)\n",
      "Collecting tensorflow-text~=2.11.0\n",
      "  Using cached tensorflow_text-2.11.0-cp39-cp39-macosx_10_9_x86_64.whl (5.6 MB)\n",
      "Collecting gin-config\n",
      "  Using cached gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: tensorflow-datasets in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tf-models-official) (4.8.0)\n",
      "Collecting tf-slim>=1.1.0\n",
      "  Using cached tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "Requirement already satisfied: tensorflow-addons in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tf-models-official) (0.19.0)\n",
      "Collecting oauth2client\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting sacrebleu\n",
      "  Using cached sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: matplotlib in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tf-models-official) (3.5.3)\n",
      "Collecting immutabledict\n",
      "  Using cached immutabledict-2.2.3-py3-none-any.whl (4.0 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp39-cp39-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "Collecting pyyaml<6.0,>=5.1\n",
      "  Using cached PyYAML-5.4.1-cp39-cp39-macosx_10_9_x86_64.whl (259 kB)\n",
      "Requirement already satisfied: six in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tf-models-official) (1.16.0)\n",
      "Collecting tensorflow-model-optimization>=0.4.1\n",
      "  Using cached tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tf-models-official) (1.9.3)\n",
      "Collecting kaggle>=1.3.9\n",
      "  Using cached kaggle-1.5.12.tar.gz (58 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting Cython\n",
      "  Using cached Cython-0.29.32-py2.py3-none-any.whl (986 kB)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tf-models-official) (0.12.0)\n",
      "Collecting seqeval\n",
      "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tensorflow~=2.11.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tf-models-official) (2.11.0)\n",
      "Collecting google-auth-httplib2>=0.1.0\n",
      "  Using cached google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Using cached httplib2-0.21.0-py3-none-any.whl (96 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Using cached google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "Collecting uritemplate<5,>=3.0.1\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.6.0)\n",
      "Requirement already satisfied: certifi in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from kaggle>=1.3.9->tf-models-official) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from kaggle>=1.3.9->tf-models-official) (2.8.2)\n",
      "Collecting python-slugify\n",
      "  Using cached python_slugify-7.0.0-py2.py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: urllib3 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from kaggle>=1.3.9->tf-models-official) (1.26.13)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from pandas>=0.22.0->tf-models-official) (2022.7)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (3.19.6)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (3.7.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (1.3.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (1.42.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (0.29.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (2.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (1.14.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (1.6.3)\n",
      "Requirement already satisfied: packaging in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (22.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (2.11.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (4.4.0)\n",
      "Requirement already satisfied: setuptools in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (65.5.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (2.1.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (0.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow~=2.11.0->tf-models-official) (14.0.6)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from matplotlib->tf-models-official) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from matplotlib->tf-models-official) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from matplotlib->tf-models-official) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from matplotlib->tf-models-official) (1.4.2)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from oauth2client->tf-models-official) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from oauth2client->tf-models-official) (0.2.8)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from oauth2client->tf-models-official) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from requests->remotezip) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from requests->remotezip) (2.0.4)\n",
      "Requirement already satisfied: lxml in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from sacrebleu->tf-models-official) (4.9.1)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting portalocker\n",
      "  Using cached portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: regex in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from sacrebleu->tf-models-official) (2022.7.9)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from seqeval->tf-models-official) (1.1.2)\n",
      "Requirement already satisfied: typeguard>=2.7 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow-addons->tf-models-official) (2.13.3)\n",
      "Requirement already satisfied: promise in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
      "Requirement already satisfied: etils[epath] in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow-datasets->tf-models-official) (0.9.0)\n",
      "Requirement already satisfied: dill in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow-datasets->tf-models-official) (0.3.6)\n",
      "Requirement already satisfied: click in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow-datasets->tf-models-official) (8.0.4)\n",
      "Requirement already satisfied: tensorflow-metadata in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow-datasets->tf-models-official) (1.12.0)\n",
      "Requirement already satisfied: toml in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow~=2.11.0->tf-models-official) (0.35.1)\n",
      "Collecting google-auth<3.0.0dev,>=1.19.0\n",
      "  Using cached google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.57.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official) (4.2.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (2.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (0.4.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (1.6.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (2.2.2)\n",
      "Requirement already satisfied: zipp in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from etils[epath]->tensorflow-datasets->tf-models-official) (3.8.0)\n",
      "Requirement already satisfied: importlib_resources in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from etils[epath]->tensorflow-datasets->tf-models-official) (5.10.1)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (4.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (2.1.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (3.2.1)\n",
      "Building wheels for collected packages: kaggle, pycocotools, seqeval\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73031 sha256=25ce228c10d7ea4e5cc1c15d2593befd882b6234a4b06ff5991bde7b4e750b6e\n",
      "  Stored in directory: /Users/wilsonbeckman/Library/Caches/pip/wheels/50/0a/6a/77a4f3a534f0e5fd0909a376bbdfc88238a43eb2ac35947dc7\n",
      "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp39-cp39-macosx_10_9_x86_64.whl size=86659 sha256=ca88e883a9546604317d58d4e48c33e10b21f6ee046b772d1a15f3ce8d891eb2\n",
      "  Stored in directory: /Users/wilsonbeckman/Library/Caches/pip/wheels/29/98/97/6c7dca1f8e4c854e15a2676ac98ae3f46ec83ee031d827a5c8\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16164 sha256=b4da0b283208a4d2adcfbfd396ca759e6ad7da52dad866e9aa3f96ebb7c7d1c2\n",
      "  Stored in directory: /Users/wilsonbeckman/Library/Caches/pip/wheels/9c/d6/00/1ccfd5a7466a94774e00022683d4b028836032dfb85007822b\n",
      "Successfully built kaggle pycocotools seqeval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: text-unidecode, sentencepiece, py-cpuinfo, gin-config, uritemplate, tf-slim, tensorflow-model-optimization, pyyaml, python-slugify, portalocker, opencv-python-headless, opencv-python, immutabledict, httplib2, Cython, colorama, sacrebleu, oauth2client, kaggle, google-auth, seqeval, pycocotools, google-auth-httplib2, google-api-core, google-api-python-client, tensorflow-text, tf-models-official\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.6.0.66\n",
      "    Uninstalling opencv-python-4.6.0.66:\n",
      "      Successfully uninstalled opencv-python-4.6.0.66\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.6.0\n",
      "    Uninstalling google-auth-2.6.0:\n",
      "      Successfully uninstalled google-auth-2.6.0\n",
      "Successfully installed Cython-0.29.32 colorama-0.4.6 gin-config-0.5.0 google-api-core-2.11.0 google-api-python-client-2.70.0 google-auth-2.15.0 google-auth-httplib2-0.1.0 httplib2-0.21.0 immutabledict-2.2.3 kaggle-1.5.12 oauth2client-4.1.3 opencv-python-4.5.2.52 opencv-python-headless-4.5.2.52 portalocker-2.6.0 py-cpuinfo-9.0.0 pycocotools-2.0.6 python-slugify-7.0.0 pyyaml-5.4.1 sacrebleu-2.3.1 sentencepiece-0.1.97 seqeval-1.2.2 tensorflow-model-optimization-0.7.3 tensorflow-text-2.11.0 text-unidecode-1.3 tf-models-official-2.11.2 tf-slim-1.1.0 uritemplate-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install remotezip tqdm opencv-python==4.5.2.52 opencv-python-headless==4.5.2.52 tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ccc98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 14:28:13.836198: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/wilsonbeckman/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import remotezip as rz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\n",
    "from official.projects.movinet.modeling import movinet\n",
    "from official.projects.movinet.modeling import movinet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2ea533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_per_class(zip_url):\n",
    "    \"\"\"\n",
    "    List the files in each class of the dataset given the zip URL.\n",
    "\n",
    "    Args:\n",
    "      zip_url: URL from which the files can be unzipped. \n",
    "\n",
    "    Return:\n",
    "      files: List of files in each of the classes.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    with rz.RemoteZip(URL) as zip:\n",
    "        for zip_info in zip.infolist():\n",
    "            files.append(zip_info.filename)\n",
    "    return files\n",
    "\n",
    "def get_class(fname):\n",
    "    \"\"\"\n",
    "    Retrieve the name of the class given a filename.\n",
    "\n",
    "    Args:\n",
    "      fname: Name of the file in the UCF101 dataset.\n",
    "\n",
    "    Return:\n",
    "      Class that the file belongs to.\n",
    "    \"\"\"\n",
    "    return fname.split('_')[-3]\n",
    "\n",
    "def get_files_per_class(files):\n",
    "    \"\"\"\n",
    "    Retrieve the files that belong to each class. \n",
    "\n",
    "    Args:\n",
    "      files: List of files in the dataset.\n",
    "\n",
    "    Return:\n",
    "      Dictionary of class names (key) and files (values).\n",
    "    \"\"\"\n",
    "    files_for_class = collections.defaultdict(list)\n",
    "    for fname in files:\n",
    "        class_name = get_class(fname)\n",
    "        files_for_class[class_name].append(fname)\n",
    "    return files_for_class\n",
    "\n",
    "def download_from_zip(zip_url, to_dir, file_names):\n",
    "    \"\"\"\n",
    "    Download the contents of the zip file from the zip URL.\n",
    "\n",
    "    Args:\n",
    "      zip_url: Zip URL containing data.\n",
    "      to_dir: Directory to download data to.\n",
    "      file_names: Names of files to download.\n",
    "    \"\"\"\n",
    "    with rz.RemoteZip(zip_url) as zip:\n",
    "        for fn in tqdm.tqdm(file_names):\n",
    "            class_name = get_class(fn)\n",
    "            zip.extract(fn, str(to_dir / class_name))\n",
    "            unzipped_file = to_dir / class_name / fn\n",
    "\n",
    "            fn = pathlib.Path(fn).parts[-1]\n",
    "            output_file = to_dir / class_name / fn\n",
    "            unzipped_file.rename(output_file,)\n",
    "\n",
    "def split_class_lists(files_for_class, count):\n",
    "    \"\"\"\n",
    "    Returns the list of files belonging to a subset of data as well as the remainder of\n",
    "    files that need to be downloaded.\n",
    "\n",
    "    Args:\n",
    "      files_for_class: Files belonging to a particular class of data.\n",
    "      count: Number of files to download.\n",
    "\n",
    "    Return:\n",
    "      split_files: Files belonging to the subset of data.\n",
    "      remainder: Dictionary of the remainder of files that need to be downloaded.\n",
    "    \"\"\"\n",
    "    split_files = []\n",
    "    remainder = {}\n",
    "    for cls in files_for_class:\n",
    "        split_files.extend(files_for_class[cls][:count])\n",
    "        remainder[cls] = files_for_class[cls][count:]\n",
    "    return split_files, remainder\n",
    "\n",
    "def download_ufc_101_subset(zip_url, num_classes, splits, download_dir):\n",
    "    \"\"\"\n",
    "    Download a subset of the UFC101 dataset and split them into various parts, such as\n",
    "    training, validation, and test. \n",
    "\n",
    "    Args:\n",
    "      zip_url: Zip URL containing data.\n",
    "      num_classes: Number of labels.\n",
    "      splits: Dictionary specifying the training, validation, test, etc. (key) division of data \n",
    "              (value is number of files per split).\n",
    "      download_dir: Directory to download data to.\n",
    "\n",
    "    Return:\n",
    "      dir: Posix path of the resulting directories containing the splits of data.\n",
    "    \"\"\"\n",
    "    files = list_files_per_class(zip_url)\n",
    "    for f in files:\n",
    "        tokens = f.split('/')\n",
    "        if len(tokens) <= 2:\n",
    "            files.remove(f) # Remove that item from the list if it does not have a filename\n",
    "\n",
    "    files_for_class = get_files_per_class(files)\n",
    "\n",
    "    classes = list(files_for_class.keys())[:num_classes]\n",
    "\n",
    "    for cls in classes:\n",
    "        new_files_for_class = files_for_class[cls]\n",
    "        random.shuffle(new_files_for_class)\n",
    "        files_for_class[cls] = new_files_for_class\n",
    "\n",
    "    # Only use the number of classes you want in the dictionary\n",
    "    files_for_class = {x: files_for_class[x] for x in list(files_for_class)[:num_classes]}\n",
    "\n",
    "    dirs = {}\n",
    "    for split_name, split_count in splits.items():\n",
    "        print(split_name, \":\")\n",
    "        split_dir = download_dir / split_name\n",
    "        split_files, files_for_class = split_class_lists(files_for_class, split_count)\n",
    "        download_from_zip(zip_url, split_dir, split_files)\n",
    "        dirs[split_name] = split_dir\n",
    "\n",
    "    return dirs\n",
    "\n",
    "def format_frames(frame, output_size):\n",
    "    \"\"\"\n",
    "    Pad and resize an image from a video.\n",
    "\n",
    "    Args:\n",
    "      frame: Image that needs to resized and padded. \n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted frame with padding of specified output size.\n",
    "    \"\"\"\n",
    "    frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "    frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "    return frame\n",
    "\n",
    "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
    "    \"\"\"\n",
    "    Creates frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "    \"\"\"\n",
    "    # Read each video frame by frame\n",
    "    result = []\n",
    "    src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "    video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "    if need_length > video_length:\n",
    "        start = 0\n",
    "    else:\n",
    "        max_start = video_length - need_length\n",
    "        start = random.randint(0, max_start + 1)\n",
    "\n",
    "    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "    # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "    ret, frame = src.read()\n",
    "    result.append(format_frames(frame, output_size))\n",
    "\n",
    "    for _ in range(n_frames - 1):\n",
    "        for _ in range(frame_step):\n",
    "            ret, frame = src.read()\n",
    "        if ret:\n",
    "            frame = format_frames(frame, output_size)\n",
    "            result.append(frame)\n",
    "        else:\n",
    "            result.append(np.zeros_like(result[0]))\n",
    "    src.release()\n",
    "    result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "    return result\n",
    "\n",
    "class FrameGenerator:\n",
    "    def __init__(self, path, n_frames, training = False):\n",
    "        \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "          Args:\n",
    "            path: Video file paths.\n",
    "            n_frames: Number of frames. \n",
    "            training: Boolean to determine if training dataset is being created.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.n_frames = n_frames\n",
    "        self.training = training\n",
    "        self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "        self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "    def get_files_and_class_names(self):\n",
    "        video_paths = list(self.path.glob('*/*.avi'))\n",
    "        classes = [p.parent.name for p in video_paths] \n",
    "        return video_paths, classes\n",
    "\n",
    "    def __call__(self):\n",
    "        video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "        pairs = list(zip(video_paths, classes))\n",
    "\n",
    "        if self.training:\n",
    "            random.shuffle(pairs)\n",
    "\n",
    "        for path, name in pairs:\n",
    "            video_frames = frames_from_video_file(path, self.n_frames) \n",
    "            label = self.class_ids_for_name[name] # Encode labels\n",
    "            yield video_frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f6c1d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 300/300 [01:20<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 200/200 [00:56<00:00,  3.52it/s]\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://storage.googleapis.com/thumos14_files/UCF101_videos.zip'\n",
    "download_dir = pathlib.Path('./UCF101_subset/')\n",
    "subset_paths = download_ufc_101_subset(URL, \n",
    "                        num_classes = 10, \n",
    "                        splits = {\"train\": 30, \"test\": 20}, \n",
    "                        download_dir = download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6176755b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 14:31:17.407493: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_frames = 8\n",
    "\n",
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], num_frames, training = True),\n",
    "                                          output_signature = output_signature)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], num_frames),\n",
    "                                         output_signature = output_signature)\n",
    "test_ds = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb991ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 7 0 5 2 0 5 5], shape=(8,), dtype=int16)\n",
      "tf.Tensor([8 8 8 0 6 2 8 8], shape=(8,), dtype=int16)\n",
      "tf.Tensor([8 1 8 8 8 8 3 1], shape=(8,), dtype=int16)\n",
      "tf.Tensor([1 3 8 4 6 0 2 6], shape=(8,), dtype=int16)\n",
      "tf.Tensor([4 6 7 1 2 5 5 8], shape=(8,), dtype=int16)\n",
      "tf.Tensor([5 4 5 9 0 2 0 8], shape=(8,), dtype=int16)\n",
      "tf.Tensor([5 6 8 7 9 8 6 0], shape=(8,), dtype=int16)\n",
      "tf.Tensor([6 2 9 9 1 1 3 7], shape=(8,), dtype=int16)\n",
      "tf.Tensor([5 4 4 1 5 5 4 9], shape=(8,), dtype=int16)\n",
      "tf.Tensor([7 9 0 5 7 6 4 5], shape=(8,), dtype=int16)\n"
     ]
    }
   ],
   "source": [
    "for frames, labels in train_ds.take(10):\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a3f6db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (8, 8, 224, 224, 3)\n",
      "Label: (8,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape: {frames.shape}\")\n",
    "print(f\"Label: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe14f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = layers.GRU(units = 4, return_sequences = True, return_state = True)\n",
    "inputs = tf.random.normal(shape = [1, 10, 8])\n",
    "result, state = gru(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb2f7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "first_half, state = gru(inputs[:, :5, :])\n",
    "second_half, _ = gru(inputs[:, 5:, :], initial_state = state)\n",
    "\n",
    "print(np.allclose(result[:, :5, :], first_half))\n",
    "print(np.allclose(result[:, 5:, :], second_half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3911ccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x movinet_a0_base/\r\n",
      "x movinet_a0_base/checkpoint\r\n",
      "x movinet_a0_base/ckpt-1.data-00000-of-00001\r\n",
      "x movinet_a0_base/ckpt-1.index\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fc8d0912cd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = 'a0'\n",
    "resolution = 224\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "backbone = movinet.Movinet(model_id = model_id)\n",
    "backbone.trainable = False\n",
    "\n",
    "model = movinet_model.MovinetClassifier(backbone = backbone, num_classes = 600)\n",
    "model.build([None, None, None, None, 3])\n",
    "\n",
    "!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n",
    "!tar -xvf movinet_a0_base.tar.gz\n",
    "\n",
    "checkpoint_dir = f'movinet_{model_id}_base'\n",
    "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "checkpoint = tf.train.Checkpoint(model = model)\n",
    "status = checkpoint.restore(checkpoint_path)\n",
    "status.assert_existing_objects_matched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1c84414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
    "    \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
    "    model = movinet_model.MovinetClassifier(\n",
    "        backbone = backbone, \n",
    "        num_classes = num_classes)\n",
    "    model.build([batch_size, num_frames, resolution, resolution, 3])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d06749",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_classifier(batch_size, num_frames, resolution, backbone, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fccecf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(loss = loss_obj, optimizer = optimizer, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc269e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(train_ds,\n",
    "                    validation_data=test_ds,\n",
    "                    epochs=num_epochs,\n",
    "                    validation_freq=1,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0908ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10503f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_predicted_labels(dataset):\n",
    "    \"\"\"\n",
    "    Create a list of actual ground truth values and the predictions from the model.\n",
    "\n",
    "    Args:\n",
    "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
    "\n",
    "    Return:\n",
    "      Ground truth and predicted values for a particular dataset.\n",
    "    \"\"\"\n",
    "    actual = [labels for _, labels in dataset.unbatch()]\n",
    "    predicted = model.predict(dataset)\n",
    "\n",
    "    actual = tf.stack(actual, axis=0)\n",
    "    predicted = tf.concat(predicted, axis=0)\n",
    "    predicted = tf.argmax(predicted, axis=1)\n",
    "\n",
    "    return actual, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc215c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
    "    cm = tf.math.confusion_matrix(actual, predicted)\n",
    "    ax = sns.heatmap(cm, annot=True, fmt='g')\n",
    "    sns.set(rc={'figure.figsize':(12, 12)})\n",
    "    sns.set(font_scale=1.4)\n",
    "    ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
    "    ax.set_xlabel('Predicted Action')\n",
    "    ax.set_ylabel('Actual Action')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    ax.xaxis.set_ticklabels(labels)\n",
    "    ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7eec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = FrameGenerator(subset_paths['train'], num_frames, training = True)\n",
    "label_names = list(fg.class_ids_for_name.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447168d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predicted = get_actual_predicted_labels(test_ds)\n",
    "plot_confusion_matrix(actual, predicted, label_names, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b08f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
